\chapter{Discriminante Lineal de Fisher}
\label{ch:chapter2}

En este capítulo se resolverá el problema del discriminante lineal de Fisher que se introdujo en el capítulo anterior. Se han presentado distintas formulaciones en libros de aprendizaje estadístico y clasificación de patrones, pero muchas veces no se resuelve el problema original debido a su complejidad computacional. \cite{wang2007trace}\cite{ngo2012trace} Problemas como maximizar la traza de la matriz de dispersión entre clases sujeto a una restricción sobre la matriz de dispersión interna, maximizar la traza del cociente de matrices; o bien, el cociente de determinantes son planteados en distintos textos \cite{duda2012pattern} \cite{hastie2009elements} \cite{mitchell2006discipline} \cite{fukunaga2013introduction}, pero estos resultan ser versiones simplificadas del problema. 

En la primer parte del capítulo se enuncia la teoría correspondiente para seguir con facilidad la tesis. En la segunda parte se proporciona los problemas inherentes al planteamiento: la generalización a $p$ dimensiones, las condiciones bajo las cuales tiene solución existe, la conversión a un problema escalar y la localización del óptimo. Como se muestra en capítulos posteriores el problema se resolverá con métodos iterativos que resultan ser computacionalmente accesibles. 



\section{Matrices de dispersión}
El problema en cuestión hace amplio uso de las matrices de dispersión, en específico de la matriz de covarianza, la matriz de dispersión de todos los individuos, la matriz de dispersión interna y la matriz de dispersión entre clases. Es importante analizar a profundidad la terminología y las fórmulas que se usarán a lo largo de la tesis para entender la lógica detrás del discriminante de Fisher.

Sea $\Sigma$ la matriz de covarianza (\textit{Covariance Matrix)} de todos los individuos. Se define como $\widehat{\Sigma}$ al estimador insesgado de $\Sigma$ el cual está escalada entre $N-1$:

\begin{equation} \label{eq:2.1}
\widehat{\Sigma} = \frac{1}{N-1} \sum_{i=1}^{N}(x_i - \mu)(x_i - \mu)^T	
\end{equation}


Si esta matriz no está escalada por $N-1$ entonces se le conoce como matriz de dispersión (\textit{Scatter Matrix}), en esta tesis se representará como $S_T$, con el subíndice $T$ que significa que está tomando en cuenta a todos los individuos:

\begin{equation} \label{eq:2.2}
S_T = \sum_{i=1}^{N}(x_i - \mu)(x_i - \mu)^T	
\end{equation}

Cuando solo se toman a los individuos de una clase particular $k$, se puede encontrar su correspondiente matriz de dispersión, representada como $S_k$, con el subíndice $k$ simbolizando que está tomando en cuenta solo a los individuos de la clase $k$:

\begin{equation*}
S_k = \sum_{\substack{i=1 \\ y_i = k}}^{N} (x_k - \mu)(x_k - \mu)^T	
\end{equation*}

De esta manera se define la matriz de dispersión interna (\textit{Within-class scatter matrix}) como la suma sobre $k$ de todas las matrices de dispersión de cada clase:

\begin{equation}\label{eq:2.3}
S_I = \sum_{k=1}^{K} 
					\sum_{\substack{i = 1\\
                  			   	y_i = k}}
                    ^{N}
 ({x_i-\mu_{k}})({x_i-\mu_{k}})^T 	
\end{equation}

Ahora solo falta definir la matriz de dispersión entre clases (\textit{Between-class scatter matrix}) como la suma de diferencias al cuadrado de las medias de clase contra la media de todos los datos multiplicada por el número de individuos en cada clase $N_k$:

\begin{equation} \label{eq:2.4}
S_E = \sum_{k = 1}^K N_k (\mu_k - \mu)(\mu_k - \mu)^T	
\end{equation}

\begin{figure}[!ht]
  \centering
	\includegraphics[width=1\textwidth]{Figures/Chapter2_SE_SI}	
  \caption[Distancias en las matrices de dispersión]
  {En la gráfica (1) se representa la $S_E$, es decir las distancias al cuadrado entre la media de todos los datos (Punto negro) y las medias de cada clase (Puntos de color gruesos). La gráfica (2) representa $S_I$; es decir, la distancia al cuadrado de los individuos a la media de su clase. La gráfica (3) representa $S_T$, la dispersión de los datos con respecto a la media de todos.}
\end{figure}

Entre la matriz de dispersión interna $S_I$, la matriz de dispersión entre clases $S_E$ y la matriz de dispersión total $S_T$ existe una relación importante. Se cumple que $S_T = S_I + S_E $; es decir, la dispersión de las medias de grupos con la media global más la dispersión de cada clase individual es igual a la dispersión de los datos sin la información de las clases. Los datos de la figura (1.1) representan las distancias que toman en cuenta cada una de estas matrices. Para ejemplificar esta relación se generaron 10 datos por clase suponiendo distribuciones normales (El coeficiente de correlación de los datos generados es $-0.005$):

\begin{center}
\begin{tabular}{ c c c}
\toprule
\textbf{Clase} & \textbf{Distribución x1} & \textbf{Distribución x2} \\
\midrule\\
\addlinespace[-2ex]
1 & N(-5, 2.5) & N(-8, 2)\\
2 & N(-3, 2.5) & N(-4, 2)\\
3 & N(-7, 2.5) & N(-1, 2) \\
\addlinespace[1.5ex]
\bottomrule
\end{tabular}
\end{center}

Calculando las matrices de dispersión de acuerdo a las fórmulas (\ref{eq:2.2}), (\ref{eq:2.3}), (\ref{eq:2.4}):

\begin{center}
\begin{tabular}{ c c c}
\toprule
\textbf{$S_I$} & \textbf{$S_E$} & \textbf{$S_T$} \\
\midrule\\
\addlinespace[-2ex]
$ \begin{bmatrix}  186.05 & 2.78 \\ 2.78 &  94.58 \end{bmatrix}$ &
$ \begin{bmatrix} 46.13 & -4.15 \\ -4.15 & 234.57 \end{bmatrix}$ &
$ \begin{bmatrix}  232.18 & -1.36 \\ -1.36 &  329.16 \end{bmatrix}$ \\
\addlinespace[1.5ex]
\bottomrule
\end{tabular}
\end{center}

De este ejemplo numérico se puede ver que al sumar la dispersión interna $S_I$ y la dispersión entre clases $S_E$ da como resultado la dispersión de todos los individuos $S_T$. En general este resultado se cumple, por lo que a continuación se demuestra esta relación:

\pagebreak

\begin{proposition} \label{lemma2.1}
Sea $S_E$ la matriz de dispersión entre clases, $S_I$ la matriz de dispersión interna y $S_T$ la matriz de dispersión de los datos, entonces se tiene que cumplir la siguiente igualdad: $S_T$ = $S_I$ + $S_E$
\end{proposition}

\begin{proof}
\begin{align*}
S_T =& \sum_{i = 1}^{N} (x_i - \mu)(x_i - \mu)^T \\
    =& \sum_{k = 1}^{K} \sum_{\substack{i=1 \\ y_i = k}}^N  (x_i -\mu_k + \mu_k-\mu)(x_i -\mu_k + \mu_k- \mu)^T \\
    =& \sum_{k = 1}^{K} \sum_{\substack{i=1 \\ y_i = k}}^N  [(x_i -\mu_k)(x_i-\mu_k)^T + 2 (x_i-\mu_k)(\mu_k-\mu)^T +\\
     & (\mu_k -\mu)(\mu_k-\mu)^T ]\\
    =& \sum_{k = 1}^{K} \sum_{\substack{i=1 \\ y_i = k}}^N (x_i -\mu_k)(x_i-\mu_k)^T + 
    2  \sum_{k = 1}^{K} \sum_{\substack{i=1 \\ y_i = k}}^N  (x_i-\mu_k)(\mu_k-\mu)^T  + \\ 
     & \sum_{k = 1}^{K} \sum_{\substack{i=1 \\ y_i = k}}^N  (\mu_k -\mu)(\mu_k-\mu)^T
\end{align*}

pero se tiene que $\sum_{k = 1}^{K} \sum_{\substack{i=1 \\ y_i = k}}^N  (x_i-\mu_k)(\mu_k-\mu)^T = 0$ ya que $\sum_{\substack{i=1 \\ y_i = k}}^N  (x_i-\mu_k) = 0$. Así que la expresión $S_T$ se simplifica a:

\begin{align*}
S_T =& \sum_{k = 1}^{K} \sum_{\substack{i=1 \\ y_i = k}}^N (x_i -\mu_k)(x_i-\mu_k)^T + 
       \sum_{k = 1}^{K} \sum_{\substack{i=1 \\ y_i = k}}^N  (\mu_k -\mu)(\mu_k-\mu)^T \\
      =& \sum_{k = 1}^{K} \sum_{\substack{i=1 \\ y_i = k}}^N (x_i -\mu_k)(x_i-\mu_k)^T +
       \sum_{k = 1}^{K}  N_k  (\mu_k -\mu)(\mu_k-\mu)^T \\
      =& S_I + S_E
\end{align*}
\end{proof}



Un problema muy común que surge en problemas de aprendizaje estadístico es que el costo computacional puede volverse intratable conforme la dimensionalidad de los individuos crece. En el Discriminante Lineal de Fisher se requiere hacer el cómputo de las matrices de dispersión de los individuos constantemente (o bien calcular la inversa de matrices de alta dimensionalidad), cálculos que para grandes dimensiones son sumamente costosos. Existen distintas maneras para hacer frente a este problema, uno de ellos involucra el PCA (Principal Component Analysis). Este método es fácil de calcular y solo requiere computar una vez la de matriz de dispersión \cite{ngo2012trace}. Debido a la finalidad de esta tesis no se profundizará en más técnicas para hacer frente a este problema, pero en textos como \cite{hastie2009elements}, \cite{duda2012pattern} aparecen distintos métodos para reducción de dimensionalidad.


Retomando el problema de cociente de trazas, lo que se busca es encontrar la proyección que mantenga juntos individuos de una clase al mismo tiempo que separa las medias de distintas clases. Una vez obtenida esta proyección se puede encontrar un hiperplano separador de los datos, o bien algún criterio para asignar la clase de pertenencia. 

\begin{equation}\label{eq:2.5}
	\max_{\substack{V \in {\rm I\!R}^{n \times p} \\ V^TC V = I}} \frac{Tr(V^T S_E V)}{Tr(V^T S_I V)} 	
\end{equation}

 La solución a este problema no tiene una forma cerrada, por lo que en la literatura se buscan formulaciones alternas para resolverlo de una manera más sencilla \cite{wang2007trace} \cite{fukunaga2013introduction}, algunos ejemplos de estas formulaciones son:

\begin{equation}\label{eq:2.6}
	\max_{\substack{V \in {\rm I\!R}^{n \times p} \\ V^T S_I V = I}} Tr(V^T S_E V)
\end{equation}

\begin{equation} \label{eq:2.7}
	\max_{\substack{V \in {\rm I\!R}^{n \times p} \\ V^TC V = I}} Tr\left( \frac{V^T S_E V}{V^T S_I V}\right) 	
\end{equation}

\begin{equation} \label{eq:2.8}
	\max_{\substack{V \in {\rm I\!R}^{n \times p} \\ V^TC V = I}} \frac{|V^T S_E V|}{|V^T S_I V|} 	
\end{equation}

Con $|\bullet| = det(\bullet)$ y $Tr(\bullet) = Traza(\bullet)$.


En la siguiente parte de este capítulo se resolverá el problema original (\ref{eq:2.5}) para $p = 1$, para lo cual se introduce el cociente generalizado de Rayleigh. Para la generalización a $p$ dimensiones solo se plantea el problema y se proporcionan los casos en que la solución existe y es única. Seguido de esto se definirá una función $f(\rho)$ la cual sirve para encontrar el óptimo por métodos iterativos. Por último haciendo uso de los eigenvalores de $S_I$ y $S_E$ se darán cotas inferiores y superiores al óptimo.



% \section{Formulaciones alternas al cociente de trazas}

% Las formulaciones alternas que se han propuesto en la literatura tienen la característica que tener soluciones cerradas; es decir, se sabe que tiene una solución analítica y hay una manera de calcularla. La forma para encontrar esta solución involucra cálculo, en particular las funciones derivadas del determinante y de la traza, por lo que se enuncian a continuación \cite{petersen2008matrix}:

% Sean $V$ no cuadrada, $A$ y $B$ simétricas:

% \begin{equation}
% \frac{\partial{|V^T A V|}}{\partial{V}}	= 2 |V^T A V| AV (V^T A V)^-1
% \end{equation}

% \begin{equation}
% \frac{\partial{Tr(V^T B V)}}{\partial{V}}	= 2BV
% \end{equation}


% También se ocuparán dos propiedades de los eigenvalores usadas comúnmente:

% \begin{equation}
% 	Tr(A) = \sum_{i=1}^p \lambda_i
% \end{equation}

% \begin{equation}
% 	|A| = \prod_{i=1}^p \lambda_i
% \end{equation}

% Con $\lambda_i$ los eigenvalores de la matriz $A$

% Se comenzará introduciendo el problema asociado a los componentes principales. Para la resolución de este son comúnmente usados tres planteamientos distintos, pero en este texto se ocupará la formulación de máxima varianza \footnote{Las otras dos formulaciones que son comunes son la de Mínimo error y PCA probabilístico.}.Sea $S$ la matriz de dispersión de los datos y $V = \big[ v_1 | v_2 | ... | v_p \big]$, entonces el problema de optimización asociado a esta formulación es:

% \begin{equation}
% 	\max_{\substack{V \in {\rm I\!R}^{nxp} \\ V^T V = I}} Tr(V^T S V)
% \end{equation}

% La solución al problema es tomar $V$ como los eigenvectores de la matriz $S$, con lo cual la función objetivo toma el valor de:
% \begin{equation}
% 	Tr(V^T S V) = \sum_{i = 1}^p \lambda_i	
% \end{equation}

% Con $\lambda_1, \lambda_2,... ,\lambda_n$ los eigenvalores de la matriz $S$, de manera que $\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_n$. Es decir, proyectamos los datos sobre los eigenvectores de la matriz $S$ (Para mayor detalles consultar Apéndice A).

% \subsection{Problema de traza de matriz de dispersión entre clases}

% Este planteamiento es propuesto en diversos textos \cite{hastie2009elements}. Este problema difiere en la restricción impuesta y en la función objetivo a maximizar, se toma $S_E$ en lugar de $S$ y se restringe por $V^T S_I V = I$.

% \begin{equation*}
% 	\max_{\substack{V \in {\rm I\!R}^{nxp} \\ V^T S_I V = I}} Tr(V^T S_E V)
% \end{equation*}

% \begin{proposition}
% La solución al problema de maximizar la traza entre clases:

% \begin{equation*}
% 	\max_{\substack{V \in {\rm I\!R}^{nxp} \\ V^T S_I V = I}} Tr(V^T S_E V)
% \end{equation*}

% Es equivalente a la solución del problema de eigenvalores generalizado de la matriz $(S_I^{-1} S_E)$; es decir, $V$ es igual a los eigenvalores de $(S_I^{-1} S_E)$. 

% \end{proposition}


% \begin{proof}
% La solución a la primer formulación alterna (\ref{eq:2.2}) se encuentra calculando el lagrangiano asociado al problema de maximización: 

% $$\mathcal{L}(V;\lambda) = Tr(V^T S_E V) - \lambda_{P_1} (V^T S_I V - 1) $$

% $$\frac{\partial \mathcal{L}(V;\lambda_1)}{\partial V} = 2 S_E V - 2 \lambda_{P_1} S_I V = 0$$

% $$\frac{\partial \mathcal{L}(V;\lambda_1)}{\partial V}= (S_I^{-1} S_E) V - \lambda_{P_1} V = 0$$

% \begin{equation}
%  	\frac{\partial \mathcal{L}(V;\lambda_1)}{\partial V}= (S_I^{-1} S_E) V = \lambda_{P_1} V	
% \end{equation}

% \end{proof}

% % Más adelante se demostrará que para el caso donde $V \in {\rm I\!R}^{nx1}$, la solución de este problema es equivalente a la solución del cociente de trazas. 

% \section{Problema de traza del cociente (Ratio trace problem)}

% Este problema es abordado en varios documentos que tratan del tema, el cual encuentra encuentra iterativamente la solución $v_i$ a 
% $\frac{v_i^T S_E v_i}{v_i^T S_I v_i}$ para $i = 1, ..., p$. Por este motivo se le considera un algoritmo goloso (\textit{greedy}) \cite{wang2007trace}. 


% \begin{proposition}
% La solución al problema de traza del cociente (Ratio trace problem) es la siguiente:

% \begin{equation*}
% 	\max_{\substack{V \in {\rm I\!R}^{nxp} \\ V^TC V = I}} Tr(\frac{V^T S_E V}{V^T S_I V}) 	
% \end{equation*}

% es equivalente al problema de eigenvalores generalizado de la matriz $S_I^{-1} S_E$.
% \end{proposition}


% \begin{proof}
% $$	\max_{\substack{V \in {\rm I\!R}^{nxp} \\ V^TC V = I}} Tr(\frac{V^T S_E V}{V^T S_I V}) 	$$

% $$J(V) = Tr(\frac{V^T S_E V}{V^T S_I V})$$

% $$\frac{\partial J(V)}{\partial V} = 2 S_E V (V^T S_I V)^{-1} - 2 S_I V (V^T S_E V) (V^T S_I V)^{-2}= 0$$

% $$\frac{\partial J(V)}{\partial V} = S_E V - S_I V(V^T S_E V)(V^T S_I V) = 0 $$

% $$\frac{\partial J(V)}{\partial V} = (S_I^{-1}S_E) V - V (V^T S_E V)(V^T S_I V)^{-1} = 0 $$

% \begin{equation} 
% \frac{\partial J(V)}{\partial V} = (S_I^{-1}S_E) V = V \lambda_{P_2}
% \end{equation}

% \end{proof}
% Es sencillo de ver que este problema es equivalente al problema de optimización del cociente de trazas cuando $V \in {\rm I\!R}^{nx1}$ ya que ambos problemas resultan tener valores escalares en el numerador y denominador, de manera que se maximiza $\frac{V^T S_E V}{V^T S_I V}$ en ambos casos.




% \subsection{Problema del cociente de determinantes (Ratio determinant problem)}

% La idea intuitiva de este problema es maximizar el cociente de determinantes. El determinante es la multiplicación de los eigenvalores de la matriz, entonces es como medir el volumen de la hiperesfera que contienen a los datos. \cite{duda2012pattern}. Este planteamiento es común en el tema de análisis discriminante de Fisher. \cite{wang2007trace}\cite{duda2012pattern}. 

% \begin{proposition}
% La solución al problema de cociente de determinantes es la siguiente:
% \begin{equation*}
% 	\max_{\substack{V \in {\rm I\!R}^{nxp} \\ V^TC V = I}} \frac{|V^T S_E V|}{|V^T S_I V|} 	
% \end{equation*}
% es equivalente al problema de eigenvalores generalizado de la matriz $S_I^{-1} S_E$.
% \end{proposition}

% \begin{proof}
% $$	\max_{\substack{V \in {\rm I\!R}^{nxp} \\ V^TC V = I}} \frac{|V^T S_E V|}{|V^T S_I V|} 	$$

% $$J(V) = \frac{|V^T S_E V|}{|V^T S_I V|}$$

% $$\frac{\partial J(V)}{\partial V} = 
% 2 |V^T S_E V| |V^T S_I V| S_E V (V^T S_E V)^{-1} - $$ 
% $$ 2 |V^T S_I V|^{-2} |V^T S_I V| |V^T S_E V| S_I V (V^T S_I V)^{-1}=0
% $$

% $$ \frac{\partial J(V)}{\partial V} = S_E V (V^T S_E V)^{-1}- S_I V (V^T S_I V)^{-1}=0$$

% $$ \frac{\partial J(V)}{\partial V} = (S_I^{-1}S_E) V = V (V^T S_I V)^{-1}(V^T S_E V)$$

% \begin{equation}
%  \frac{\partial J(V)}{\partial V} = (S_I^{-1}S_E) V = V \lambda_{P_3}
% \end{equation}
% \end{proof}

% De nuevo es sencillo ver que es equivalente al problema de optimización del cociente de trazas cuando $V \in {\rm I\!R}^{nx1}$, ya que el numerador y el denominador resultan ser valores escalares, de manera que en ambos casos se maximiza $\frac{V^T S_E V}{V^T S_I V}$.

% Los tres planteamientos llevan a resolver el problema generalizado de eigenvalores de la matriz $S_I^{-1} S_E$ el cual tiene una forma cerrada \cite{wang2007trace} \cite{duda2012pattern}. Pero existen otros planteamientos que también crean una solución exacta, pero que no necesariamente involucran los eigenvalores de $S_I^{-1} S_E$ \cite{fukunaga2013introduction}.


\section{Problema del cociente de trazas}

El problema del cociente de trazas (Trace ratio problem) es fácil de ver cuando $V \in {\rm I\!R}^{n \times p}$ proyecta a un espacio de pocas dimensiones. Por ejemplo, cuando $p = 2$ se desea obtener la mejor proyección sobre un plano y cuando $p = 1$ sobre una recta. Para ejemplificar esta situación se creo un conjunto sintético donde cada $x_i \in {\rm I\!R}^{3}$. Las distribuciones son normales y se proyectan en ${\rm I\!R}^{2}$ y ${\rm I\!R}^{1}$.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{Figures/Chapter2_1} 
  \caption[Mejores proyecciones en ${\rm I\!R}^{2}$ y ${\rm I\!R}$]
  {En la gráfica de arriba se muestran los datos originales en 
   ${\rm I\!R}^{3}$ los cuales fueron generados a través de distribuciones normales con distintas medias y varianzas. En la gráfica de abajo a la izquierda se muestra la mejor proyección en ${\rm I\!R}^{2}$ y abajo a la derecha la mejor proyección en ${\rm I\!R}$}
\end{figure}

\subsection{Solución cuando p = 1}

El problema \ref{eq:2.5} toma la siguiente forma cuando $V \in {\rm I\!R}^{n}$. Se nombrara $v$ a este proyector a una dimensión ya que resulta ser solo un vector:


\begin{equation} \label{eq:2.9}
\max_{v \in {\rm I\!R}^{n}} \frac{v^T S_E v}{v^T S_I v}  
\end{equation}

Se tiene que $x_i \in {\rm I\!R}^{n}$ son los individuos originales con $i = 1 , ... , N $. Entonces sean $w_i \in {\rm I\!R}$ los individuos proyectados por el vector $v$ de manera que $w_i = v^Tx_i$. De esta manera es conveniente definir $\widehat{\mu}_k = v^T \mu_k$ y $\widehat{\mu} = v^T \mu$ como la media por clase y la media total de los datos proyectados.

\textbf{Matriz entre clases $\Phi_{E}$ de los individuos proyectados $w_i$:}

$$\Phi_{E} = \sum\limits_{k =1}^{K} N_k (\widehat{\mu}_k - \widehat{\mu} )^2$$

$$\Phi_{E} = \sum\limits_{k =1}^{K} N_k (v^T \mu_k - v^T \mu)^2$$

$$\Phi_{E} =  \sum\limits_{k =1}^{K} N_k v^T ( \mu_k - \mu )(\mu_k - \mu )^T v $$
Por distributividad en matrices se cumple que $vAv+vBv =  v(A+B)v$, entonces:

\begin{equation}\label{eq:2.10}
\Phi_{E} = v^T \big[ \sum\limits_{k =1}^{K} N_k ( \mu_k - \mu )(\mu_k - \mu )^T \big] v	
\end{equation}

\textbf{Matriz intra clase $\Phi_{I}$ de los individuos proyectados $w_i$}:
$$\Phi_{I} = \sum\limits_{k = 1}^{K} \sum\limits_{\substack{i = 1\\
                            y_i = k}}^{N} (w_i - \widehat{\mu}_k)^2 $$

$$\Phi_{I} = \sum\limits_{k = 1}^{K} \sum\limits_{\substack{i = 1\\
                            y_i = k}}^{N} (v_i^T x_i - v_i^T \mu_k)^{2} $$
$$\Phi_{I} =  \sum\limits_{k = 1}^{K} \sum\limits_{\substack{i = 1\\
                            y_i = k}}^{ N} v^T( x_i - \mu_k) ( x_i - \mu_k)^T v  $$

Usando de nuevo la distibutividad de matrices:

\begin{equation}\label{eq:2.11}
\Phi_{I} = v^T \big[ \sum\limits_{k = 1}^{K} \sum\limits_{\substack{i = 1\\
                            y_i = k}}^{ N} ( x_i - \mu_k) ( x_i - \mu_k)^T \big] v	
\end{equation}


 Las fórmulas de $\Phi_{I}$ y $\Phi_{E}$ de los individuos $w_i$ se pueden expresar en función de las matrices de dispersión interna y entre clases $S_I$ y $S_E$ de los individuos originales $x_i$. De esta manera:

 $$\Phi_{E} = f(S_E) = v^T S_E v$$
 $$\Phi_{I} = f(S_I) = v^T S_I v$$

Se tiene que $\Phi_{I}, \Phi_{E} \in {\rm I\!R}$, entonces maximizar el cociente $\frac{\Phi_{E}}{\Phi_{I}}$ con respecto a $v$ tiene como resultado una proyección que conserva cerca a los individuos pertenecientes a la misma clase, mientras que aleja a los centros de cada clase. Para el caso de una dimensión se puede encontrar una solución cerrada. La teoría asociada a este problema de maximización esta relacionada con el Cociente de Rayleigh y el Cociente Generalizado de Rayleigh.

Sea $A$ una matriz simétrica, entonces el cociente de Rayleigh y el Cociente Generalizado de Rayleigh tienen las siguientes formulaciones respectivamente:


\begin{equation}\label{eq:2.12}
\frac{v^T A v}{v^T v}
\end{equation}

\begin{equation}\label{eq:2.13}
\frac{v^T A v}{v^T B v}
\end{equation}

Un problema común que surge en machine learning es la maximización de este cociente con respecto a $v$. Problemas como PCA, LDA y sus generalizaciones invocan a las fórmulas \ref{eq:2.12} \ref{eq:2.13}. Por este motivo se dará la solución cuando $v \in {\rm I\!R}^{n}$.

\begin{proposition} \label{lemma2.2}
La solución a la maximización del cociente de Rayleigh:
$$\max_{v \in {\rm I\!R}^{n} } \frac{v^T A v}{v^Tv} $$
cuando $A$ es simétrica, es obtenida cuando $v$ es el eigenvector asociado al eigenvalor más grande de la matriz $A$.
\end{proposition}


\begin{proof}
Primero se hace la observación que factorizando $c$ las siguientes dos formulaciones de la maximización son equivalentes cuando $c \neq 0$ y $c \in {\rm I\!R}$: 

$$\max_{v \in {\rm I\!R}^{n} } \frac{v^T A v}{(v^T v)} $$
$$\max_{v \in {\rm I\!R}^{n} } \frac{(cv)^T A (cv)}{(cv)^T(cv)} $$

Sin pérdida de generalidad se supone que $||v|| = 1$. Cuando $v \in {\rm I\!R}^{n \times 1}$ el problema es equivalente a la maximización del numerador sujeta a una restricción de igualdad $v^T v = 1$:

$$\max_{\substack{v \in {\rm I\!R}^{n}\\ v^T v = 1 }} v^T A v $$

Este problema es más sencillo y se puede resolver formulando la función lagrangiana asociado, derivando e igualando a $0$:

$$\mathcal{L}(v, \lambda) = v^T A v - \lambda(v^T v -1)$$
$$\frac{\partial \mathcal{L}(v, \lambda)}{\partial v} = (A + A^T) v - 2\lambda v = 0$$

Como $A$ es simétrica, entonces $A + A^T = 2A$, por lo que la solución es:

\begin{equation}\label{eq:2.14}
{\partial v} = A v = \lambda v	
\end{equation}


Este es el problema de eigenvalores generalizado. Entonces el máximo es alcanzado cuando $v$ es el eigenvector asociado al eigenvalor más grande de $A$.

\end{proof}

Una vez enunciado este resultado, se procede a resolver el cociente generalizado de Raleigh, el cual esta intimamente relacionado con el discriminante lineal de Fisher para un proyector unidimensional.


\begin{proposition}\label{lemma2.3}
Sean $S_E$ y $S_I$ matrices simétricas definidas positivas. La solución óptima al discriminante lineal de Fisher para un proyector unidimensional $v \in {\rm I\!R}^{1}$ es el eigenvector asociado al eigenvalor más grande del problema generalizado de eigenvalores $(S_I)^{-1}S_E v = \lambda v$.
\end{proposition}

\begin{proof}
La maximización del discriminante lineal de Fisher de la forma \ref{eq:2.9} es equivalente al Cociente de Rayleigh Generalizado de $S_E$ relativo a $S_I$. Planteando el problema y suponiendo que $v$ proyecta a un espacio unidimensional se tiene que \cite{hastie2009elements}:

$$  \max_{v \in {\rm I\!R}^{n} } \frac{v^T S_E v}{v^T S_I v}  $$

Como $S_I$ es positiva definida entonces se puede encontrar su factorización de Cholesky, de manera que $S_I = P^T P$ y se define $v' = P v$ y $C= P^{-T} S_E P^{-1} $, entonces el problema de maximización \ref{eq:2.9} se puede escribir como \cite{hastie2009elements}:

$$\max_{v \in {\rm I\!R}^{n} } \frac{v^T S_E v}{v^T P^T P v} $$

$$\max_{v \in {\rm I\!R}^{n} } \frac{v'^T P^{-T}S_E P^{-1}v'}{(v'^T)(v')} $$

$$\max_{v \in {\rm I\!R}^{n} } \frac{v'^T C v'}{v'^Tv'} $$

Como $v$ es un vector, se tiene que $||v||^2_2 = v_1^2 + v_2^2 + ... + v_n^2 = v^T v$, con esta igualdad se representa la maximización anterior como:

$$ \max_{v \in {\rm I\!R}^{n} }
\left(\frac{v'}{\left\Vert{v'}\right\Vert}\right)^T C \left(\frac{v'}{\left\Vert{v'}\right\Vert}\right) $$

Es decir, el problema original es equivalente a una forma cuadrática restringida a la esfera unitaria en otro sistema de coordenadas; es decir, es equivalente al problema de maximización:


\begin{equation}\label{eq:2.15}
\max_{v \in {\rm I\!R}^{n}} v^T S_E v  \quad sujeto \enspace a \quad v^T S_I v = 1	
\end{equation}


 Maximizar el cociente se puede resolver con la proposición \ref{lemma2.2}. Es así como se obtiene que la solución del planteamiento original es la solución del problema de eigenvalores generalizado 

$$C v' = \lambda v'$$

Para encontrar la solución en términos de $S_E$ y $S_I$ se sustituye en la solución obtenida $v' = Pv$ y $C = P^{-T} S_E P^{-1}$

$$C v' = \lambda v'$$
$$P^{-T} S_E P^{-1} P v = \lambda P v$$
$$P^T P^{-T} S_E v = \lambda P^T P v$$
$$ S_E v = \lambda S_I v $$

\begin{equation} \label{eq:2.16}
(S_I)^{-1}S_E v = \lambda  v 	
\end{equation}
 
De donde se obtiene que es equivalente al problema generalizado de eigenvalores de $(S_I)^{-1} S_E$.

\end{proof}

\subsection{Generalización a p dimensiones}

Para dimensiones más grandes de $v$ la solución planteada en el capítulo anterior no funciona, ya que no es equivalente al problema de maximización planteado. Esto genera la dificultad de no tener una solución cerrada, por lo que se han propuesto métodos iterativos y planteamientos alternos a la solución.

La generalización a $p$ dimensiones implica que los individuos $x_i \in {\rm I\!R}^{n}$ son proyectados ahora por la matriz $V = (V_1 | V_2 | ... |V_p)$, de manera que $w_i = V^T x_i$ con $w_i \in {\rm I\!R}^{p}$ y $V_j \in {\rm I\!R}^{n}$. De esta manera las matrices $\Phi_I$ y $\Phi_E$ se definen como sigue:

\begin{equation*}
\Phi_E = \sum\limits_{k = 1}^{K} N_{k} ||\widehat{\mu}_k - \widehat{\mu}||_2^2
\end{equation*}

\begin{equation*}
\Phi_E = \sum\limits_{k = 1}^{K} N_{k} ||V^T \mu_k - V^T \mu||_2^2
\end{equation*}

\begin{equation*}
\Phi_E = \sum\limits_{k = 1}^{K} N_{k} ||V^T (\mu_k - \mu)||_2^2
\end{equation*}


\begin{equation}\label{eq:2.17}
  \Phi_E = \sum\limits_{k = 1}^{K} N_{k} \big[ (V_1^T (\mu_k - \mu))^2 + (V_2^T (\mu_k - \mu))^2+ ... + (V_p^T (\mu_k - \mu))^2 \big]
\end{equation}

De esta expresión hay que destacar que $V_1^T (\mu_k - \mu)$ es un escalar, ya que $V_1 \in {\rm I\!R}^n$ y $(\mu_k - \mu) \in {\rm I\!R}^{n}$. Otra fórmula equivalente y que es comúnmente usada por sus propiedades algebraicas consiste en la siguiente expresión:

\begin{equation}\label{eq:2.18}
\Phi_E = \sum\limits_{k = 1}^{K} N_{k} Tr \big[ V^T (\mu_k - \mu) (\mu_k - \mu)^T V \big]	
\end{equation}

Para ejemplificarla se toma una clase $k = k_1$.  Al desarrollar $(\bullet) = V^T (\mu_1 - \mu) (\mu_1 - \mu)^T V$ se tiene una matriz en ${\rm I\!R}^{p \times p}$ igual a:


\begin{equation*}
(\bullet)= \left(\!
    \begin{array}{c}
      V_1^T (\mu_1-\mu)\\
      V_2^T (\mu_1-\mu)\\
      \vdots \\
      V_p^T (\mu_1-\mu)
    \end{array}
  \!\right) 
  \left(\!\begin{array}{c}
      (\mu_1-\mu)^T V_1 \quad
      (\mu_1-\mu)^T V_2 \quad
      \hdots \quad
      (\mu_1-\mu)^T V_p
    \end{array}
  \!\right) 
\end{equation*} 

\begin{equation*}
(\bullet)= \left(\!
    \begin{array}{ccc}
      V_1^T (\mu_1-\mu) (\mu_1-\mu)^T V_1 & \hdots & V_1^T (\mu_1-\mu) (\mu_1-\mu)^T V_p  \\
      V_2^T (\mu_1-\mu) (\mu_1-\mu)^T V_1 & \hdots & V_2^T (\mu_1-\mu) (\mu_1-\mu)^T V_p  \\
      \vdots & \ddots & \vdots\\
      V_p^T (\mu_1-\mu) (\mu_1-\mu)^T V_1 & \hdots & V_p^T (\mu_1-\mu) (\mu_1-\mu)^T V_p
    \end{array}
  \!\right) 
\end{equation*} 


\begin{equation*}
(\bullet)= \left(\!
    \begin{array}{ccc}
      (V_1^T (\mu_1-\mu))^2 & \hdots & V_1^T (\mu_1-\mu) (\mu_1-\mu)^T V_p \\
       V_2^T (\mu_1-\mu) (\mu_1-\mu)^T V_1  & \hdots & V_2^T (\mu_1-\mu) (\mu_1-\mu)^T V_p  \\
      \vdots & \ddots & \vdots\\
      V_p^T (\mu_1-\mu) (\mu_1-\mu)^T  & \hdots & (V_p^T (\mu_1-\mu))^2
    \end{array}
  \!\right) 
\end{equation*} 

 Por lo tanto al calcular la traza de la matriz de $p \times p$ desarrollada arriba, se tiene que $Tr(V^T (\mu_1 - \mu) (\mu_1 - \mu)^T V)$ es equivalente a:
 \begin{equation*}
Tr(\bullet) = (V_1^T (\mu_1-\mu))^2+ (V_2^T (\mu_1-\mu) )^2 + \hdots + (V_p^T (\mu_1-\mu) )^2
 \end{equation*}

Al generalizar a las $K$ clases y usando la propiedad de linealidad en la traza; es decir, $Tr(A+B) = Tr(A)+Tr(B)$, entonces se puede escribir de la siguiente manera:

\begin{equation*}
\Phi_E = Tr \sum\limits_{k = 1}^{K} N_{k}  \big[ V^T (\mu_k - \mu) (\mu_k - \mu)^T V \big]
\end{equation*}


Esta expresión es equivalente a \ref{eq:2.17}. Como paso final se factoriza $V^T$ y $V$ sobre todos los sumandos, lo que nos llevaría a lo siguiente:

\begin{equation*} 
\Phi_E =  Tr (V^T \sum\limits_{k = 1}^{K} N_{k}  \big[(\mu_k - \mu) (\mu_k - \mu)^T \big] V)   
\end{equation*}

o, expresada en términos de $S_E = \sum\limits_{k = 1}^{K} N_{k}  \big[(\mu_k - \mu) (\mu_k - \mu)^T \big]$

\begin{equation}\label{eq:2.19}
\Phi_E =  Tr (V^T S_E V)     
\end{equation}

Similarmente se puede llegar a la formulación de la varianza intra-clase = 
\begin{equation}\label{eq:2.20}
\Phi_{I} =  Tr (V^T S_I V )
\end{equation}



\subsection{Existencia de la solución}

Para demostrar la existencia y unicidad de la solución, las matrices $S_I$ y $S_E$ deben cumplir ciertas características. Sean $A = S_E$ y $B = S_I$, la primer condición que se les impone es que sean positivas definidas. La razón que apoya la restricción está relacionada con la forma de la función a maximizar, que es un cociente. Como $B$ se encuentra en el denominador, se tiene que evitar que $Tr(V^T B V) = 0$, ya que con este valor se indetermina la función objetivo \cite{ngo2012trace}. 

T.T. Ngo propone generalizar el estudio a las matrices positivas semidefinidas. Para esto se deben encontrar los casos en que $Tr(V^T B V)$ toma el valor de $0$. Si se diagonaliza a la matriz $B = Q \Lambda_{B} Q^T$ con $Q$ ortogonal y $\Lambda_{B}$ una matriz diagonal con entradas iguales a los eigenvalores de $B$, entonces:

\begin{equation*}
Tr(\Lambda_{B}) = \lambda_{B_1}+ \lambda_{B_2}+ ... +\lambda_{B_n} \qquad con \qquad \widehat{V} = Q^T V
\end{equation*}



De este modo $\widehat{V} = (\widehat{V}_1 | \widehat{V}_2 | ... | \widehat{V}_p)$ y cada $\widehat{V}_i^T = (\widehat{V}_{i1}, \widehat{V}_{i2}, ..., \widehat{V}_{in})$ es un vector renglón. De esta manera la matriz $\widehat{V}^T$:

\begin{equation*}
\widehat{V}^T = 	
\left(\!
    \begin{array}{c}
      \widehat{V}_1^T\\
      \widehat{V}_2^T\\
      \vdots \\
      \widehat{V}_p^T
    \end{array}
  \!\right)   = 
\left(\!
    \begin{array}{cccc}
      \widehat{V}_{11} & \widehat{V}_{12} & \hdots & \widehat{V}_{1n}\\
      \widehat{V}_{21} & \widehat{V}_{22} & \hdots & \widehat{V}_{2n}\\
      \vdots &  \vdots &\ddots & \vdots\\
      \widehat{V}_{p1} & \widehat{V}_{p2} & \hdots & \widehat{V}_{pn}\\
    \end{array}
  \!\right) 
\end{equation*}

Por lo que la traza que involucra a $B$ tiene la siguiente forma:

\begin{equation*}
Tr(V^T B V) = Tr(V^T Q \Lambda_{B} Q^T V) 	
\end{equation*}

\begin{equation*}
Tr(V^T B V)  = Tr(\widehat{V}^T \Lambda_{B} \widehat{V})
\end{equation*}

\begin{equation*}
V^T B V = 
\left(\!
    \begin{array}{c}
      \widehat{V}_1^T\\
      \widehat{V}_2^T\\
      \vdots \\
      \widehat{V}_p^T
    \end{array}
  \!\right) 
  \left(\!
    \begin{array}{cccc}
      \lambda_{B_1} & 0 & \hdots & 0\\
      0 & \lambda_{B_2} & \hdots & 0\\
      \vdots &  \vdots &\ddots & \vdots\\
      0 & 0 & \hdots & \lambda_{B_n}\\
    \end{array}
  \!\right) 
  \left(\!\begin{array}{c}
      \widehat{V}_1 |
      \widehat{V}_2 |
      \hdots |
      \widehat{V}_p
    \end{array}
  \!\right) 
\end{equation*} 

Desarrollando la multiplicación de matrices, y calculando la traza resulta en los siguientes sumandos:
 
 \begin{equation*}
\begin{aligned}
      Tr(V^T B V) =& \lambda_{B_1} \widehat{V}_{11}^2&  +
                     & \lambda_{B_2} \widehat{V}_{12}^2& +
                     & \hdots& +
                     &\lambda_{B_n} \widehat{V}_{1n}^2& + \\
                     & \lambda_{B_1} \widehat{V}_{21}^2&+
                     & \lambda_{B_2} \widehat{V}_{22}^2& +
                     & \hdots& +
                     &\lambda_{B_n} \widehat{V}_{2n}^2& + \\
                     & \vdots&  
                     & \vdots& 
                     & \vdots& 
                     & \vdots& \\
                     & \lambda_{B_1} \widehat{V}_{p1}^2&+
                     & \lambda_{B_2} \widehat{V}_{p2}^2& +
                     & \hdots& + 
                     & \lambda_{B_n} \widehat{V}_{pn}^2.&  
 \end{aligned}
 \end{equation*}

 Es fácil de ver que la expresión de arriba tiene $p \times n$ sumandos, por lo que se puede expresar en términos de dos sumatorias. La primera de $j=1,...,p$ y la segunda de $i = 1,...n$:

\begin{equation*} 
Tr(V^T B V) = \sum \limits_{j=1}^{p} \sum\limits_{i=1}^{n} \lambda_{B_i} \widehat{V}_{ji}^2
\end{equation*}

\begin{equation}\label{eq:2.21}
Tr(V^T B V) = \sum\limits_{i=1}^{n} \lambda_{B_i} \sum \limits_{j=1}^{p} \widehat{V}_{ji}^2    
\end{equation}

De la última expresión se separa la sumatoria sobre $i$. De esta manera, para cada elemento $i$ se tienen dos factores:

\begin{equation}\label{eq:2.22}
(i) \lambda_{B_i}
\end{equation}

 \begin{equation}\label{eq:2.23}
 (ii) \sum \limits_{j=1}^{p} \widehat{V}_{ji}^2   
 \end{equation}
 

 La idea para que $Tr(V^T B V)$ sea positivo, es que al menos uno de los sumandos sea positivo. Si \ref{eq:2.22} y \ref{eq:2.23} son ambos distintos de cero para al menos una $i$, entonces se cumple esta condición. Esta idea está expresada en el Lema \ref{lemma2.4}.

\begin{lemma}\label{lemma2.4}
Sea $B$ positiva semidefinida y $V \in {\rm I\!R}^{n\times p}$. Si $B$ tiene a lo más $p-1$ eigenvalores iguales a $0$, entonces $Tr(V^T B V)  = Tr(\widehat{V}^T \Lambda_{B} \widehat{V}) \neq 0$  para cualquier matriz ortogonal $V$.
\end{lemma}

\begin{proof}
Sea $\widehat{V} = [\widehat{V}_1 | ... | \widehat{V}_p]$ tal que $\widehat{V} \widehat{V} = V^T Q Q^T V = V^T I_n V  = I_p$. De esta manera se puede construir una matriz $\widehat{V}' \in {\rm I\!R}^{p \times p}$ seleccionando $p$ de los $n$ renglones de $\widehat{V}$ tal que $\widehat{V}'$ sea no singular. $\widehat{V}'$ tiene la propiedad de no contener eigenvalores iguales a 0; como consecuencia, sus renglones y columnas no contienen al vector $\widehat{0}$. Al no contenerlo,se sabe que al menos hay $p$ renglones de $\widehat{V}$ tal que $\sum_{j=1}^{p}\widehat{V}_{ji}^2 \neq 0$ para cada uno de ellos. Por otra parte en el lema se supone que la matriz $B$ tiene a lo más $p-1$ eigenvalores iguales $0$ por lo que al menos un elemento de la sumatoria es distinto de cero.
\end{proof}

Analizando a mayor profundidad el resultado anterior, se sabe que hay $n-p+1$ eigenvalores de $B$ positivos ($\lambda_{B_i} \neq 0$) y $p$ renglones de $\widehat{V}$ que tienen norma distinta de cero. Al calcular la sumatoria (\ref{eq:2.21}), se tiene que al menos una combinación de $\lambda_{B_i}$ y uno de los $p$ renglones cumplen que su multiplicación tiene signo positivo. Para ejemplificar esta situación sean $C_i$ con $i = 1, ... , n-p+1$ los eigenvalores de $B$ y $K_j$ con $j = 1, ... , p$ la norma de los renglones de $\widehat{V}$ que son distintos de $0$.
 
\begin{center}
\begin{tabular}{ | c | c|  c | c|} 
\hline
$i$ & $\lambda_{B_i}$ & $\sum \limits_{j=1}^{p} \widehat{V}_{ji}^2$  & $\lambda_{B_i} \sum \limits_{j=1}^{p} \widehat{V}_{ji}^2$ \\ 
\hline
\hline
1 & $C_1$ & $0$ & $0$ \\ 
\hline
2 & $C_2$ & $0$ & $0$ \\ 
\hline
\vdots & \vdots & \vdots & \vdots \\ 
\hline
$n-p$ & $C_{n-p}$ & $0$ &  $0$\\ 
\hline
$n-p+1$ & $C_{n-p+1}$ & $K_{p}$ &  $C_{n-p+1} K_{p}$\\ 
\hline
$n-p+2$ & $0$ & $K_{p-1}$  & $0$ \\ 
\hline
\vdots & \vdots & \vdots & \vdots  \\ 
\hline
$n-1$ & $0$ & $K_2$ & $0$ \\ 
\hline
$n$ & $0$ & $K_1$ & $0$ \\ 
\hline
\hline

\end{tabular}
\end{center}

Con esta combinación se tiene que al menos hay un sumando de $\sum\limits_{i=1}^{n} \lambda_{B_i} \sum \limits_{j=1}^{p} \widehat{V}_{ji}^2 $ distinto de cero, por lo que $Tr(V^T B V) \neq 0$. Bajo estas condiciones se garantiza que el denominador sea mayor a 0, solo falta asegurarse que el numerador sea menor a infinito.

\begin{lemma}
Sea $U_p = \{ V \in {\rm I\!R}^{n \times p} | V^T V = I_p \} $ un conjunto compacto con $V = (v_1, v_2, ... , v_p)$
\end{lemma}
\begin{proof}
Se tiene que $U_p$ es un conjunto cerrado porque contiene a todos sus puntos límite; por otro lado, $U_p$ también es acotado bajo la norma 2 y la norma de Frobenius:

Tomando la norma-2 y la norma de Frobenius de $V$: 
\begin{equation*}
\begin{aligned}
	||V||_2 =& Max \{||V_x ||_2 \quad | \quad ||x||_2 = 1 \} \\
		    =& ||V_x||^2_2  \\
		    =& (Vx)^T (Vx) \\
		    =& x^T V^T V x\\
		    =& x^T x = 1\\
	||V||_F	=& \sum\limits_{F}^{p} ||v_i|| = p   
\end{aligned}
\end{equation*}

Entonces se tiene $U_p$ es cerrado y acotado, por lo que $U_p$ es compacto.
\end{proof}

Con este resultado se tiene que $Tr(V^T A V)$ toma un valor finito ya que todas sus entradas son finitas. 


\begin{lemma}\label{lemma2.5}
Sean $A$ y $B$ dos matrices simétricas tales que $B$ es positiva semidefinida con rango mayor que $n-p$; es decir, que tenga al menos $n-p+1$ eigenvalores distintos de cero. Entonces el cociente $\ref{eq:2.5}$ admite un máximo con valor $\rho^*$ \cite{ngo2012trace}.
\end{lemma}

\begin{proof}
Tomando el resultado del lema \ref{lemma2.4} se tiene que $Tr(V^T B V) \neq 0$; por otra parte, $V \in U_p$ que es un conjunto compacto. Con estas dos observaciones, el valor de \ref{eq:2.5} es distinto de infinito. Entonces el cociente $\ref{eq:2.5}$ admite un máximo con valor $\rho^*$ y que tiene como argumento $V^{**}$.
\end{proof}

\subsection{Equivalencia con un problema escalar}

\textbf{Valor en el óptimo}. Del lema \ref{lemma2.5} se sabe que existe una matriz $V^{**} \in U_p$ tal que \ref{eq:2.5} alcanza el valor máximo $\rho^*$. Expresando esta idea se tiene que:

\begin{equation} \label{eq:2.24}
 \frac{Tr(V^{T**} A V^{**})}{Tr(V^{T**} B V^{**})} = \rho^* 
\end{equation}

Entonces para cualquier otra matriz $V \in U_p $:

\begin{equation}\label{eq:2.25}
 \frac{Tr(V^T A V)}{Tr(V^T B V)} \leq \rho^* 
\end{equation}

Como la traza es un operador lineal y por la propiedad distributiva de las matrices entonces (\ref{eq:2.25})es equivalente a:

\begin{equation*}
 Tr(V^T A V)- \rho^* Tr(V^T B V) \leq 0 
\end{equation*}

\begin{equation*}
	Tr(V^T A V- \rho^* V^T B V) \leq 0	
 \end{equation*}

 \begin{equation} \label{eq:2.26}
	Tr(V^T (A - \rho^* B )V) \leq 0	
 \end{equation}

y el resultado equivalente para (\ref{eq:2.24}):
	
 \begin{equation} \label{eq:2.27}
	Tr(V^{T**} (A - \rho^* B )V^{**}) = 0	
 \end{equation}


Para facilitar la lectura, de aquí en adelante se define la función $G(\rho) = A- \rho B$. Maximizar el lado izquierdo de la desigualdad (\ref{eq:2.26}) sujeto a $V^T V = I$ es equivalente a maximizar un problema de eigenvalores generalizado. Usando lo establecido en el apéndice A, se sabe que el valor máximo de este problema dado $\rho^*$:

\begin{equation}\label{eq:2.28}
	\max_{\substack{V \in {\rm I\!R}^{n \times p} \\ V^T V = I}} Tr(V^T G(\rho^*) V) = \lambda_{G(\rho^*)_1} + \lambda_{G(\rho^*)_2} + ... + \lambda_{G(\rho^*)_p}
\end{equation}
  
 Con $\lambda_{G(\rho^*)_1} \geq \lambda_{G(\rho^*)_2} \geq ... \geq \lambda_{G(\rho^*)_p}$ los $p$ eigenvalores más grandes de $G(\rho^*)$. De esta manera el valor óptimo de \ref{eq:2.28} es simplemente la suma de los $p$ eigenvalores más grandes de esta matriz, y $V^{**}$ el conjunto de correspondientes eigenvectores. Para obtener este valor y la matriz, el primer paso es encontrar a $\rho^*$, ya que teniéndolo es inmediato calcular $V^{**}$. Dada esta premisa, se puede ver que el problema a resolver se reduce a buscar el valor óptimo de $\rho$. Para esto se define la función $f(\rho)$ sobre todo $\rm I\!R$, tal que $f(\rho)$ es continua sobre su argumento $\rho$: 

\begin{equation}  \label{eq:2.29}
	f(\rho) = \max_{V^T V = I} Tr(V^T (G(\rho)) V)
\end{equation}

Es conveniente examinar $f(\rho)$ con dos objetivos, el primero es estimar la dificultad de calcular el valor de $f(\rho)$ y el segundo es encontrar la maximización adecuada para obtener $\rho^*$. Respecto al primer punto, la manera de calcular $f(\rho)$ en cada punto es equivalente a \ref{eq:2.28}, pero en lugar de usar los eigenvalores de $G(\rho^*)$ se usan los de $G(\rho)$. En particular se llamará $V(\rho)^*$ al argumento que resuelve (\ref{eq:2.29}) \footnote{Se utilizó la nomenclatura de $V(\rho^*)$ porque estos eigenvectores dependen del valor de $\rho$ en la matriz $A - \rho B$.}. Sean $\lambda_{G(\rho)_1} \geq \lambda_{G(\rho)_2} \geq ... \geq \lambda_{G(\rho)_n}$ los $n$ eigenvalores de $G(\rho)$. Con esta notación $f(\rho)$ toma el valor de:


\begin{equation}\label{eq:2.30}
f(\rho) = \lambda_{G(\rho)_1} + \lambda_{G(\rho)_2} + ... +\lambda_{G(\rho)_p}
\end{equation}


Para el segundo punto, la idea es iterar hasta obtener el valor de $\rho^*$. Por esto, es conveniente analizar como se comporta la función con respecto a su argumento. A continuación se presentan dos propiedades de $f(\rho)$. Para demostrarlas, primero se enuncia el teorema 8.1.5 de \cite{golub2012matrix}.

\begin{theorem} \label{teorem.1}
	
	Sean $X$ y $X+E$ matrices simétricas $n \times n$, y $\lambda_{X_k}$, $\lambda_{E_k}$ los k-ésimos eigenvalores más grandes de $X$ y $E$ respectivamente. De esta manera $\lambda_{X_k}$ es el k-ésimo eigenvalor más grande de $X$ y $\lambda_{E_1}$ el más grande de $E$. Con estas definiciones se cumple lo siguiente:

	\begin{equation}\label{eq:2.31}
		\lambda_{X_k} +\lambda_{E_n} \leq \lambda_{(X+E)_k} \leq \lambda_{X_k} +\lambda_{E_1}
	\end{equation}

\end{theorem}

Se procede a enunciar este lema acerca de la función $f(\rho)$

\begin{lemma}\label{lemma2.6}
La función $f(\rho) = \max_{V^T V = I} Tr(V^T (A - \rho B) V)$ cumple las siguientes dos propiedades: 

(1) $f$ es una función no creciente de $\rho$ \\
(2) $f(\rho)= 0$  si y solo si $\rho = \rho^*$
\end{lemma}

Para probar la parte $(1)$ del lema \ref{lemma2.6} se comparan los valores de $f(\rho)$ para $\rho_1$ y $\rho_2$ con $\rho_2 \geq \rho_1$. Como se desea demostrar que $f$ es una función no creciente de $\rho$, se busca que $f(\rho_2) \leq f(\rho_1)$. Se definen las matrices $Y=X+E$ y $E = Y-X$, para después restarles $\lambda_{X_k}$, entonces (\ref{eq:2.30}) se puede escribir como:

\begin{equation*}
		\lambda_{(X)_k} +\lambda_{(Y-X)_n} \leq \lambda_{(Y)_k} \leq \lambda_{(X)_k} +\lambda_{(Y-X)_1}
\end{equation*}

\begin{equation*}
		\lambda_{(Y-X)_n} \leq \lambda_{(Y)_k} - \lambda_{(X)_k} \leq \lambda_{(Y-X)_1}
\end{equation*}

La expresión en el centro de estas desigualdades es la resta del eigenvalor k-ésimo de la matriz $X$ y $Y$, por lo que si se realiza la suma de $k = 1$ a $k = p$ se tiene lo siguiente:

\begin{equation}\label{eq:2.32}
		p \lambda_{(Y-X)_n} \leq \sum_{k=1}^{p} \lambda_{(Y)_k} - \sum_{k=1}^{p} \lambda_{(X)_k} \leq  p \lambda_{(Y-X)_1}
\end{equation}

Definiendo $X = A - \rho_2 B$ y $Y = A - \rho_1 B$, entonces (\ref{eq:2.32}) toma la siguiente forma:

\begin{equation}\label{eq:2.33}
    p \lambda_{(B(\rho_2-\rho_1))_n} \leq \sum_{k=1}^{p} \lambda_{(A - \rho_1 B)_k} - \sum_{k=1}^{p} \lambda_{(A - \rho_2 B)_k} \leq  p \lambda_{(B(\rho_2-\rho_1))_1}
\end{equation}

Retomando el resultado (\ref{eq:2.30}), y sustituyéndolo en (\ref{eq:2.33}) la desigualdad queda de la siguiente forma:

\begin{equation}\label{eq:2.34}
		p \lambda_{(B (\rho_2 - \rho_1))_n} \leq f(\rho_1) - f(\rho_2) \leq  p \lambda_{(B (\rho_2 - \rho_1))_1}
\end{equation}


En la parte izquierda de la desigualdad anterior, se puede a determinar el signo que toman los eigenvalores $\lambda_{(B (\rho_2 - \rho_1))_n}$. 
Si $(\rho_2- \rho_1) \geq 0$ entonces la matriz $(\rho_2 - \rho_1)B$ es positiva semidefinida; por lo tanto, todos sus eigenvalores son mayores o iguales a 0:

\begin{equation*}
	0 \leq p \lambda_{(B (\rho_2 - \rho_1))_n} \leq f(\rho_1) - f(\rho_2) \leq  p \lambda_{(B (\rho_2 - \rho_1))_1}
\end{equation*}
	
\begin{equation}\label{eq:2.35}
	0 \leq f(\rho_1) - f(\rho_2)
\end{equation}

De esta manera $f(\rho_2)  \leq  f(\rho_1) $ cuando $\rho_2 \geq \rho_1$

Para probar la parte (2) del lema \ref{lemma2.6}, se tiene que demostrar la condición suficiente y la condición necesaria. La demostración de la primera es inmediata, ya que cuando $\rho = \rho^*$, entonces por (\ref{eq:2.27}), $f(\rho) = 0$. Para la condición necesaria se usará (\ref{eq:2.24}) y la propiedad que la $Tr(V^T B V) >0 \enspace \forall \enspace V \in U_p$. Se demostrará que, dado $f(\rho^*) = 0$, entonces:


\begin{equation}\label{eq:2.36}
	(i) \quad f(\rho) < 0 \quad si \quad \rho>\rho^*
\end{equation}
\begin{equation}\label{eq:2.37}
	(ii) \quad f(\rho) > 0 \quad si \quad \rho<\rho^*
\end{equation}

\textbf{Caso (i) $\rho > \rho^* $}

Se tiene que las siguientes desigualdades se cumplen:
\begin{equation*}
\frac{Tr(V^T A V)}{Tr(V^T B V)} \leq \rho^* < \rho	
\end{equation*}

Por lo que es equivalente a:

\begin{equation*}
	Tr(V^T (A -\rho B)V) < 0 \quad \forall V \in U_p
\end{equation*}


De esta manera $f(\rho) < 0$ cuando $\rho > \rho^*$.

\textbf{(ii) $\rho < \rho^* $}

Retomando el resultado (\ref{eq:2.25}) y suponiendo que $\rho^* > \rho$ entonces existe una $V^{*}$ tal que:


 \begin{equation*} 
 \rho < \frac{Tr(V^{T*} A V^*)}{Tr(V^{T*} B V^*)} \leq \rho^*
 \end{equation*}

\begin{equation*}
Tr(V^{T*} A V^* - \rho V^{T*} B V^*) > 0 \quad \Longrightarrow \quad \frac{Tr(V^{T*} A V^*)}{Tr(V^{T*} B V^{*})} > \rho
\end{equation*}

En particular:

\begin{equation*}
\max_{V^TV} \frac{Tr(V^{T} A V)}{Tr(V^{T} B V)} > \rho
\end{equation*}

De esta manera $f(\rho) > 0$ cuando $\rho < \rho^*$.


Las ecuaciones (\ref{eq:2.36}) (\ref{eq:2.37}), junto con la continuidad de la funcion $f$, muestran que $f(\rho) = 0$ implica $\rho = \rho^*$ \cite{ngo2012trace}. De esta manera el problema puede ser visto como encontrar la raíz de la función $f(\rho)$.

\begin{corollary}
La función $f(\rho) = \max_{V^TV = I} Tr(V^T (A -\rho B)V)$ cumple las siguientes condiciones:

$$f(\rho) > 0  \quad \forall \quad \rho \in (-\inf, \rho^*)$$
$$f(\rho) < 0  \quad \forall \quad \rho \in (\rho^*, \inf)$$

\end{corollary}

\begin{figure}[!ht]
  \centering
	\includegraphics[width=1\textwidth]{Figures/Chapter2_fp}	
  \caption[Comportamiento de $f(\rho)$]
  {La función $f(\rho)$ es no creciente para toda $\rho$. El valor de $f(\rho) = \lambda_{G(\rho)1}+ \lambda_{G(\rho)2} + ... +\lambda_{G(\rho)p}.$ $f(\rho^*) = 0 $}
\end{figure}


\begin{example} \label{ex:1}
Para ejemplificar el lema \ref{lemma2.4} se muestran las matrices $A,B \in {\rm I\!R}^{3 \times 3}$. Para el valor de $f(\rho)$ se utiliza la propiedad (\ref{eq:2.30}):

$$f(\rho) = \lambda_{G(\rho)1} + \lambda_{G(\rho)2} + ... + \lambda_{G(\rho)p}$$

con $p$ la dimensión a la que se va a proyectar. 


\begin{equation*}
A = \left(\!
    \begin{array}{ccc}
      4 & 0 & 0 \\
      0 & 6 & 0 \\
      0 & 0 & 8 
    \end{array}
  \!\right), \quad
B = \left(\!
    \begin{array}{ccc}
      1.5 & 0 & 0 \\
      0 & 2.5 & 0 \\
      0 & 0 & 5 
    \end{array}
\!\right) 
\end{equation*}



\begin{equation*}
G(\rho) = A- \rho B = \left(\!
    \begin{array}{ccc}
      4-1.5\rho & 0 & 0 \\
      0 & 6-2.5\rho & 0 \\
      0 & 0 & 8-5\rho 
    \end{array}   	
      \!\right) 
\end{equation*}

Los eigenvalores de esta matriz son $4-1.5\rho$,  $6-2.5\rho$ y  $8-5\rho$. Las funciones graficadas de estos eigenvalores son los siguientes:

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{Figures/Chapter2_3eigen}  
  \caption{Grafica de los eigenvalores en función de $\rho$}
\end{figure}


Cuando se desea que el proyector sea de dimensión 1, entonces se tiene que $f(\rho)$ es el eigenvalor más grande,  cuando sea de dimensión 2, la suma de los dos más grandes y así respectivamente. El valor de $f(\rho)$ para estos tres casos está representado en las siguientes gráficas:

\pagebreak

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{Figures/Chapter2_grid3eigen}  
  \caption{f(p) para proyectores de 1,2 y 3 dimensiones}
\end{figure}

\pagebreak


\end{example}

En este ejemplo se puede ver fácilmente que cualquier proyector $f(\rho)$ es estrictamente decreciente.






\subsection{Localización del óptimo}

En la sección anterior se encontró que la maximización al cociente de trazas puede ser visto como el problema de encontrar la raíz de la función $f(\rho) = max_{V^T V= I} Tr(V^T(A-\rho B)V)$. Por esto, encontrar un intervalo $(\rho_1, \rho_2)$ que contenga al valor óptimo $\rho^*$ puede reducir el número de iteraciones del método. Usando el lema \ref{lemma2.6} se sabe que $f$ es una función no creciente de $\rho$. Por esta razón si se encuentra una $\rho_1$ y $\rho_2$ tal que $f(\rho_1) \geq 0$ y $f(\rho_2) \leq 0$ y con la propiedad de continuidad de la funcion $f(\rho)$ entonces se encontró un intervalo que contiene a $\rho^*$. 


En esta tesis se dan cotas para el valor de $\rho^*$, la primera en función los eigenvalores de una transformación de $B -\rho A$ y la segunda en función de los eigenvalores de $B$ y $A$ \cite{ngo2012trace}. La demostración de cada una requiere del conocimiento del concepto de inercia y del Teorema de la Inercia de Sylvester. Por este motivo se presentan a continuación: \footnote{La demostración de este teorema puede ser encontrada en \cite{golub2012matrix}.}

\begin{definition}
La inercia de una matriz simétrica $A$ es la tripleta de enteros no negativos $(m, z, p)$ donde $m$, $z$ y $p$ son respectivamente el número de eigenvalores negativos, cero y positivos de $A$ \cite{golub2012matrix}.
\end{definition}

\begin{theorem}\label{teorem.2}
Sea $A \in {\rm I\!R}^{n \times n}$ una matriz simétrica y $Z \in {\rm I\!R}^{n \times n}$ no singular. Entonces $A$ y $Z^T A Z$ tienen la misma inercia \cite{golub2012matrix}.
\end{theorem}

\begin{proposition}
La raíz $\rho^*$ de $f(\rho)$ está localizada en el intervalo $(\lambda_p, \lambda_1)$ donde $\lambda_p$ es el p-ésimo eigenvalor más grande de $Z^T(A-\rho B)Z$.
\end{proposition}

\begin{proof}
Sea $Z$ la matriz que diagonaliza a $A-\rho B$ de manera que\footnote{El calculo de esta matriz puede obtenerse en el algoritmo 8.7.1 de \cite{golub2012matrix}}:

\begin{equation}\label{eq:2.38}
\begin{aligned}
 Z^T AZ = \Lambda \\ Z^T B Z = I
 \end{aligned}
\end{equation}

Con $\Lambda$ una matriz diagonal y $\mu_1, \ldots,  \mu_n$ sus respectivos eigenvalores e $I$ la identidad de tamaño $n$. Entonces por el teorema \ref{teorem.2} se sabe que $A- \rho B$ y $Z^T(A- \rho B)Z = \Lambda -\rho I$ tienen el mismo número de eigenvalores positivos, negativos y cero. Por lo tanto, la matriz en cuestión es de la siguiente forma:

\begin{equation}\label{eq:2.39}
\Lambda - \rho I = 
\left(\!
    \begin{array}{cccc}
      \mu_1 & 0 & \hdots & 0\\
      0 & \mu_2 & \hdots & 0\\
      \vdots & \vdots & \ddots & \vdots \\
      0 & 0 & \hdots & \mu_n
    \end{array}
  \!\right) - \rho
  \left(\!
    \begin{array}{cccc}
      1 & 0 & \hdots & 0\\
      0 & 1 & \hdots & 0\\
      \vdots & \vdots & \ddots & \vdots \\
      0 & 0 & \hdots & 1
    \end{array}
  \!\right) 
\end{equation} 

Como la  matriz $V$ de \ref{eq:2.5} es de tamaño $n \times p$, solo nos interesa saber el signo de los $p$ eigenvalores más grandes. Tomando $\rho = \mu_p$ entonces los elementos de la diagonal de la matriz $\Lambda- \rho I$ son de la forma:


\begin{equation}\label{eq:2.40}
\begin{aligned}
   \mu_i - \mu_p & \geq 0  \quad para \quad i \geq p\\
   \mu_i - \mu_p & \leq 0  \quad para \quad i \leq p
\end{aligned}
\end{equation} 

Los primeros p elementos tienen la propiedad de ser no negativos, ya que $\lambda_{(A-\rho B)_1} \geq \lambda_{(A-\rho B)_2} \geq ... \geq \lambda_{(A-\rho B)_p}$. Usando el teorema \ref{teorem.2} se sabe que los primeros p eigenvalores de $A-\rho B$ también son no negativos. Por ende la suma de ellos es mayor o igual que cero. 

Por otro lado si se toma $\rho = \mu_1$. Entonces los elementos de la diagonal de la matriz \ref{eq:2.39} son de la forma:

\begin{equation}\label{eq:2.41}
   \mu_i - \mu_1  \leq 0 \quad \forall \quad i
\end{equation} 

Con $i = 1, ..., p$, cada uno de los elementos de la diagonal tiene la propiedad de ser no positivo por el mismo argumento que el caso pasado. Por lo tanto los $p$ eigenvalores más grandes de $\Lambda - \rho I$ y de $A-\rho B$ son no positivos, por lo que su suma es menor o igual que cero:

\begin{equation}\label{eq:2.42}
  \rho = \mu_p  \Rightarrow \sum_{i=1}^{p} (\mu_i- \mu_p) \geq 0 \Rightarrow f(\rho) \geq 0
\end{equation}

\begin{equation}\label{eq:2.43}
  \rho = \mu_1  \Rightarrow \sum_{i=1}^{p} (\mu_i - \mu_1) \leq 0 \Rightarrow f(\rho) \leq 0
\end{equation}

\end{proof}

\begin{example} \label{ex:2}
Tomando las matrices A,B iguales que en el ejercicio \ref{ex:1}, se puede encontrar fácilmente a la matriz $Z$:


\begin{equation*}
Z = \left(\!
    \begin{array}{ccc}
      \sqrt(\frac{1}{1.5}) & 0 & 0 \\
      0 & \sqrt(\frac{1}{2.5}) & 0 \\
      0 & 0 & \sqrt(\frac{1}{5}) 
    \end{array}
  \!\right)
\end{equation*}

Con la matriz $Z$ definida de esta manera, $Z^T A  Z = \Lambda$ y $Z^T B Z = I$ toman la siguiente forma:

\begin{equation*}
\Lambda - \rho I = 
\left(\!
    \begin{array}{ccc}
      \frac{4}{1.5} & 0  & 0\\
      0 & \frac{6}{2.5}  & 0\\
      0 & 0 & \frac{8}{5}
    \end{array}
  \!\right) - \rho
  \left(\!
    \begin{array}{ccc}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{array}
  \!\right) 
\end{equation*}

Entonces se puede encontrar un intervalo tal que $\rho^* \in \big[\rho_1, \rho_2 \big]$:

\begin{equation*}
  \begin{aligned}
\rho_1 &= \mu_p \\
\rho_2 &= \mu_1  
  \end{aligned}
\end{equation*}

Conforme el tamaño de la dimensión a proyectar cambia, las cotas son las siguientes:

\begin{equation*}
  \begin{aligned}
  p =& 1 \Rightarrow \qquad \rho_1 = \frac{4}{1.5} \quad y \quad \rho_2 = \frac{4}{1.5}\\
  p =& 2 \Rightarrow \qquad \rho_1 = \frac{4}{1.5} \quad y \quad \rho_2 = \frac{6}{2.5} \\
  p =& 3 \Rightarrow \qquad \rho_1 = \frac{4}{1.5} \quad y \quad \rho_2 = \frac{8}{5}
  \end{aligned}
\end{equation*}
 

 Estas cotas se pueden var más facil en las graficas de $f(\rho)$ para distintas $p$:
\pagebreak

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{Figures/Chapter2_grid3eigen_interv}  
  \caption{Intervalos para $\rho^*$ con p = 1,2,3}
\end{figure}
\pagebreak

\end{example}

Otro intervalo que se ha desarrollado tiene que ver con directamente con los eigenvalores de $A$ y $B$ en lugar de los obtenidos por la matriz $A-\rho B$:

\begin{proposition}
Sea $B$ positiva definida, entonces la raíz $\rho^*$ de $f(\rho)$ es tal que \cite{ngo2012trace}:

\begin{equation*}
\frac{\sum_{i = 1}^{p}\lambda_{A_i}}{\sum_{i = 1}^{p}\lambda_{B_i}} \leq \rho^* \leq \frac{\sum_{i = 1}^{p}\lambda_{(A)_i}}{\sum_{i = 1}^{p}\lambda_{(B)_{n-i+1}}}	
\end{equation*}

con $\lambda_{A_i}$ y $\lambda_{B_i}$ el i-ésimo eigenvalor más grande  de la matriz $A$ y $B$ respectivamente \cite{ngo2012trace}. 
\end{proposition}


\begin{proof}
Se tiene la propiedad que para una $p$ dada:


\begin{equation}\label{eq:2.44}
\max_{V^T V = I} Tr(V^T A V) =  Tr(V^{T*} A V^*) = \sum_{i=1}^p \lambda_{A_i }
\end{equation}
 
Con $\lambda_{A_i}$ los eigenvalores de A. Como esta $V^*$ maximiza la traza sobre $A$, entonces no necesariamente maximiza la de $B$. Al sustituirla en $Tr(V^{T} B V)$ se tiene que: 

\begin{equation}\label{eq:2.45}
Tr(V^{T*} B V^*) \leq \sum_{i=1}^p \lambda_{B_i }
\end{equation}

Con $\lambda_{B_i}$ los eigenvalores de B. Al hacer el cociente de (\ref{eq:2.44}) y (\ref{eq:2.45}) Se puede acotar inferiormente a $\rho^*$:


\begin{equation*}
  \frac{\sum_{i=1}^p \lambda_{A_i}} {\sum_{i=1}^p \lambda_{B_i}} \leq \frac{Tr(V^{T*} A V^*)}{Tr(V^{T*} B V^*)} \leq \max_{V^T V} \frac{Tr(V^{T} A V)}{Tr(V^{T} B V)} = \rho^*
\end{equation*}

Ahora falta acotarlo superiormente. Usando las siguientes propiedades que son derivadas de (\ref{eq:2.28}):

\begin{equation}\label{eq:2.46}
  Tr(V^T A V) \leq \sum_{i=1}^p \lambda_{A_i} 
\end{equation}

\begin{equation}\label{eq:2.47}
  Tr(V^T B V) \geq \sum_{i=1}^p \lambda_{B_{(n-i+1)}}
\end{equation}

La expresión \ref{eq:2.47} es la suma de los p eigenvalores más chicos de $B$. Dividiendo \ref{eq:2.46} entre \ref{eq:2.47} se tiene que para cualquier matriz ortogonal $V$:

\begin{equation}\label{eq:2.48}
   \frac{Tr(V^{T} A V)}{Tr(V^{T} B V)} \leq \frac{\sum_{i=1}^p \lambda_{A_i}}{\sum_{i=1}^p \lambda_{B_{(n-i+1)}}}
\end{equation}

En particular si se toma $V = V^{**}$ (La matriz con la que se alcanza $\rho^*)$:

\begin{equation}\label{eq:2.49}
  \rho^* = \frac{Tr(V^{T**} A V^{**})}{Tr(V^{T**} B V^{**})} \leq \frac{\sum_{i=1}^p \lambda_{A_i}}{\sum_{i=1}^p \lambda_{B_{(n-i+1)}}}
\end{equation}

\end{proof}
\begin{example}
Para ejemplificar esta cota se usará las matrices $A$ y $B$ de los dos ejemplos anteriores:


\begin{equation*}
A = \left(\!
    \begin{array}{ccc}
      4 & 0 & 0 \\
      0 & 6 & 0 \\
      0 & 0 & 8 
    \end{array}
  \!\right), \quad
B = \left(\!
    \begin{array}{ccc}
      1.5 & 0 & 0 \\
      0 & 2.5 & 0 \\
      0 & 0 & 5 
    \end{array}
\!\right) 
\end{equation*}


(i) Para $p = 1$ la cota es la siguiente:
\begin{equation*}
\begin{aligned}
  \lambda_{A_1} = 8 \qquad
  \lambda_{B_1} = 5 \qquad
  \lambda_{B_3} = 1.5
\end{aligned}
\end{equation*}

\begin{equation*}
\begin{aligned}
\rho_1 = \frac{\sum_{i = 1}^{p}\lambda_{A_i}}{\sum_{i = 1}^{p}\lambda_{B_i}}  = \frac{8}{5} \qquad
\rho_2 = \frac{\sum_{i = 1}^{p}\lambda_{A_i}}{\sum_{i = 1}^{p}\lambda_{B_{n-i+1}}}  = \frac{8}{1.5}
\end{aligned}
\end{equation*}

(ii) Para $p = 2$ la cota es la siguiente:

\begin{equation*}
\begin{aligned}
\sum_{i = 1}^{2}\lambda_{A_1}  =& 14 \qquad
\sum_{i = 1}^{2}\lambda_{B_1}  =& 7.5 \qquad
\sum_{i = 1}^{2}\lambda_{B_{3-i+1}} =& 4
\end{aligned}
\end{equation*}

\begin{equation*}
\begin{aligned}
\rho_1 = \frac{\sum_{i = 1}^{p}\lambda_{A_i}}{\sum_{i = 1}^{p}\lambda_({B_i)}}
 = \frac{14}{7.5} \qquad
\rho_2 = \frac{\sum_{i = 1}^{p}\lambda_{A_i}}{\sum_{i = 1}^{p}\lambda_{B_{n-i+1}}}  = \frac{14}{4}
\end{aligned}
\end{equation*}



(iii) Para $p = 3$ la cota es la siguiente:
\begin{equation*}
  \begin{aligned}
  \sum_{i = 1}^{3}\lambda_{A_1}  = 18 \qquad
  \sum_{i = 1}^{3}\lambda_{B_1}  = 9 \qquad
  \sum_{i = 1}^{3}\lambda_{B_{n-i+1}}  = 9
  \end{aligned}
\end{equation*}

\begin{equation*}
  \begin{aligned}
\rho_1 = \frac{\sum_{i = 1}^{p}\lambda_{A_i}}{\sum_{i = 1}^{p}\lambda_{B_i}}  = \frac{18}{9} \qquad
\rho_2 = \frac{\sum_{i = 1}^{p}\lambda_{A_i}}{\sum_{i = 1}^{p}\lambda_{B_{n-i+1}}}  = \frac{18}{9}
  \end{aligned}
\end{equation*}

\end{example}
\pagebreak
\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{Figures/Chapter2_grid3eigen_interv2}  
  \caption{Intervalos para $\rho^*$ con p = 1,2,3}
\end{figure}
\pagebreak
